# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pPg7fuj8VIYbe1OYqP_TgWV-rjx0UGWK
"""

import joblib
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, accuracy_score, confusion_matrix,
    roc_curve, roc_auc_score, precision_recall_curve, average_precision_score,
    recall_score
)
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import os

sns.set_theme(style="whitegrid")


# Load and prepare dataset
df = pd.read_csv('/content/cleaned_dataset.csv')
df = df.drop(columns=['age_group', 'bmi_category', 'glucose_category'])
X = df.drop(columns=['stroke'])
y = df['stroke']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)


# Train Logistic Regression
logreg_model = LogisticRegression(max_iter=1000, solver='liblinear')
logreg_model.fit(X_train_smote, y_train_smote)

# Predictions
y_pred = logreg_model.predict(X_test)
y_pred_proba = logreg_model.predict_proba(X_test)[:, 1]

print_header("LOGISTIC REGRESSION MODEL PERFORMANCE")

accuracy = accuracy_score(y_test, y_pred)
sensitivity = recall_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
roc_auc = roc_auc_score(y_test, y_pred_proba)
avg_precision = average_precision_score(y_test, y_pred_proba)

print(f"Accuracy: {accuracy:.4f}")
print(f"Sensitivity (Recall): {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"ROC AUC Score: {roc_auc:.4f}")
print(f"Average Precision Score: {avg_precision:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=["No Stroke", "Stroke"], zero_division=0))

# Visualization
plt.figure(figsize=(18, 5))

# Confusion Matrix
plt.subplot(131)
sns.heatmap(confusion_matrix(y_test, y_pred, normalize='true'),
            annot=True, fmt='.2f', cmap='Blues',
            xticklabels=['No Stroke', 'Stroke'],
            yticklabels=['No Stroke', 'Stroke'])
plt.title('Confusion Matrix (Normalized)')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')

# ROC Curve
plt.subplot(132)
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}', color='darkorange')
plt.plot([0, 1], [0, 1], linestyle='--', color='navy')
plt.title('ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()

# Precision-Recall Curve
plt.subplot(133)
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
plt.plot(recall, precision, label=f'AP = {avg_precision:.2f}')
plt.fill_between(recall, precision, alpha=0.3)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend()

plt.tight_layout()
plt.show()

# Bar chart for Sensitivity and Specificity
plt.figure(figsize=(8, 5))
bars = plt.bar(['Sensitivity\n(Recall)', 'Specificity'], [sensitivity, specificity], color=['#ff9999', '#66b3ff'])
plt.ylim(0, 1)
plt.title('Sensitivity and Specificity')
for bar in bars:
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02, f'{bar.get_height():.4f}', ha='center')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()